{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6d16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download dataset\n",
    "train_data = datasets.Flowers102(root='./flower-102/train', split='train', download=True)\n",
    "val_data = datasets.Flowers102(root='./flower-102/val', split='val', download=True)\n",
    "test_data = datasets.Flowers102(root='./flower-102/test', split='test', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e788e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4396b",
   "metadata": {},
   "source": [
    "### Model Implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2118089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available pretrained resnets from pytorch\n",
    "class Pretrains():\n",
    "    resnet_versions = [\n",
    "        'resnet18',\n",
    "        'resnet34',\n",
    "        'resnet50',\n",
    "        'resnet101',\n",
    "        'resnet152'\n",
    "    ]\n",
    "    vgg_versions = [\n",
    "        'vgg11',\n",
    "        'vgg11_bn',\n",
    "        'vgg13',\n",
    "        'vgg13_bn',\n",
    "        'vgg16',\n",
    "        'vgg16_bn',\n",
    "        'vgg19',\n",
    "        'vgg19_bn'\n",
    "    ]\n",
    "\n",
    "class PretrainBackbone(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet/VGG backbone\n",
    "        if config.pretrain in Pretrains.resnet_versions or config.resnet_version in Pretrains.vgg_versions:\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', config.pretrain, pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid ResNet/VGG Version. Please select from: ' \n",
    "                             + ', '.join(Pretrains.resnet_versions + Pretrains.vgg_versions))\n",
    "        \n",
    "        # Segments out only the backbone layers as list, unpacks, and load into nn.Sequential\n",
    "        backbone_layers = list(model.children())[:-1]\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class ActivationFunction(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ActivationFunction, self).__init__()\n",
    "        match config.type:\n",
    "            case 'LeakyReLU':\n",
    "                self.activation_func = nn.LeakyReLU(\n",
    "                    config.negative_slope,\n",
    "                    inplace = True\n",
    "                )\n",
    "            case 'ReLU':\n",
    "                self.activation_func = nn.ReLU(inplace = True)\n",
    "            case 'Softmax':\n",
    "                self.activation_func = nn.Softmax(dim = self.dim)\n",
    "            case _:\n",
    "                raise ValueError('Invalid activation function or not implemented')\n",
    "    def forward(self, x):\n",
    "        return self.activation_func(x)\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        modules = []\n",
    "        if config.layer_num < 1:\n",
    "            raise ValueError('Number of layers cannot be less than 1')\n",
    "        for layer_idx in range(config.layer_num):\n",
    "            # Conv2d\n",
    "            modules.append(nn.Conv2d(\n",
    "                config.in_channels if not layer_idx else config.out_channels,\n",
    "                config.out_channels,\n",
    "                kernel_size = 3,\n",
    "                padding = 1\n",
    "            ))\n",
    "            \n",
    "            # Batch Normalization\n",
    "            if config.use_batchnorm:\n",
    "                modules.append(nn.BatchNorm2d(config.out_channels))\n",
    "                \n",
    "            # Activation function, skip this step if skip_last_activation is True\n",
    "            if config.skip_last_activation and layer_idx == config.layer_num - 1:\n",
    "                break   \n",
    "            modules.append(ActivationFunction(config.activation_func))\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "# Creates a mirrored Conv2dBlock\n",
    "class RevConv2dBlock(nn.Module):\n",
    "    def __init__(self, conv2d_block):\n",
    "        super(RevConv2dBlock, self).__init__()\n",
    "        \n",
    "        # Reverses module from conv2d_block\n",
    "        modules = list(conv2d_block.sequential)\n",
    "        modules.reverse()\n",
    "        module_iterator = iter(range(len(modules)))\n",
    "        for idx in module_iterator:\n",
    "            if isinstance(modules[idx], torch.nn.modules.batchnorm.BatchNorm2d):\n",
    "                \n",
    "                # Switch order of batch and conv2d\n",
    "                modules[idx], modules[idx + 1] = modules[idx + 1], modules[idx]\n",
    "                \n",
    "                # Swap conv2d with convtranspose2d\n",
    "                modules[idx] = nn.ConvTranspose2d(\n",
    "                    modules[idx].out_channels,\n",
    "                    modules[idx].in_channels,\n",
    "                    kernel_size = modules[idx].kernel_size,\n",
    "                    stride = modules[idx].stride,\n",
    "                    padding = modules[idx].padding\n",
    "                )\n",
    "                \n",
    "                modules[idx + 1] = nn.BatchNorm2d(modules[idx].out_channels)\n",
    "                \n",
    "                # Skip next index\n",
    "                next(module_iterator)\n",
    "            \n",
    "        if isinstance(modules[0], ActivationFunction):\n",
    "            activation_func = modules.pop(0)\n",
    "            modules.append(activation_func)\n",
    "            \n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "    \n",
    "class VGGBackboneBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VGGBackboneBlock, self).__init__()\n",
    "        config.skip_last_activation = False\n",
    "        \n",
    "        # Conv2d\n",
    "        self.conv2d_block = Conv2dBlock(config)\n",
    "        \n",
    "        # Maxpool\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size=config.compression_ratio, \n",
    "            stride=config.compression_ratio\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv2d_block(x)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n",
    "    \n",
    "    def get_reverse(self):\n",
    "        return RevVGGBackconeBlock(self)\n",
    "    \n",
    "class RevVGGBackconeBlock(nn.Module):\n",
    "    def __init__(self, vgg_backbone_block):\n",
    "        super(VGGBackboneBlock).__init__()\n",
    "        # To be implemented\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        return x\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Main Conv2d block\n",
    "        main_block_config = config\n",
    "        main_block_config.layer_num = config.main_layer_num\n",
    "        main_block_config.skip_last_activation = True\n",
    "        self.main_block = Conv2dBlock(main_block_config)\n",
    "        \n",
    "        # Shortcut Conv2d block, we leave self.shortcut_block as undefined if shortcut layer depth = 0\n",
    "        if config.shortcut_layer_num:\n",
    "            shortcut_block_config = config\n",
    "            shortcut_block_config.layer_num = config.shortcut_layer_num\n",
    "            shortcut_block_config.skip_last_activation = True\n",
    "            self.shortcut_block = Conv2dBlock(shortcut_block_config)\n",
    "            \n",
    "        self.activation_func = ActivationFunction(config.activation_func)\n",
    "        \n",
    "        # Optional maxpooling layer if compression_ratio is set\n",
    "        if hasattr(config, 'compression_ratio'):\n",
    "            self.maxpool = nn.MaxPool2d(\n",
    "                kernel_size=config.compression_ratio, \n",
    "                stride=config.compression_ratio\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.main_block(x)\n",
    "        if hasattr(self, 'shortcut_block'):\n",
    "            out += self.shortcut_block(x)\n",
    "        else:\n",
    "            out += x\n",
    "            \n",
    "        out = self.activation_func(out)\n",
    "            \n",
    "        if hasattr(self, 'maxpool'):\n",
    "            out = self.maxpool(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def get_reverse(self):\n",
    "        # Get reversed version\n",
    "        return RevResidualBlock(self)\n",
    "\n",
    "class RevResidualBlock(nn.Module):\n",
    "    def __init__(self, residual_block):\n",
    "        super(RevResidualBlock, self).__init__()\n",
    "        self.main_block = RevConv2dBlock(residual_block.main_block)\n",
    "        \n",
    "        if hasattr(residual_block, 'shortcut_block'):\n",
    "            self.shortcut_block = RevConv2dBlock(residual_block.shortcut_block)\n",
    "            \n",
    "        if hasattr(residual_block, 'maxpool'):\n",
    "            self.upsample = nn.Upsample(scale_factor=residual_block.maxpool.stride)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'upsample'):\n",
    "            x = self.upsample(x)\n",
    "        else:\n",
    "            x = x\n",
    "            \n",
    "        out = self.main_block(x)\n",
    "        \n",
    "        if hasattr(self, 'shortcut_block'):\n",
    "            out += self.shortcut_block(x)\n",
    "        else:\n",
    "            out += x\n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        modules = []\n",
    "        match config.type:\n",
    "            case 'residual_blocks':\n",
    "                for idx, block_feature in enumerate(config.features):\n",
    "                    if not idx:\n",
    "                        in_channels = config.in_channels\n",
    "                        out_channels = block_feature\n",
    "                    else:\n",
    "                        in_channels = config.features[idx - 1]\n",
    "                        out_channels = block_feature\n",
    "\n",
    "                    block_config = ml_collections.ConfigDict({\n",
    "                        'main_layer_num': config.main_layer_num,\n",
    "                        'shortcut_layer_num': config.shortcut_layer_num,\n",
    "                        'in_channels': in_channels,\n",
    "                        'out_channels': out_channels,\n",
    "                        'use_batchnorm': config.use_batchnorm,\n",
    "                        'activation_func': config.activation_func,\n",
    "                    })\n",
    "                    if hasattr(config, 'compression_ratio'):\n",
    "                        block_config.compression_ratio = config.compression_ratio\n",
    "\n",
    "                    modules.append(ResidualBlock(block_config))\n",
    "            case 'vgg_backbone_blocks':\n",
    "                # To be implemented\n",
    "                raise NotImplementedError('To be implemented')\n",
    "                \n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, arg):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Initialize by mirroring encoder\n",
    "        if isinstance(arg, Encoder):\n",
    "            encoder = arg\n",
    "            modules = list(encoder.sequential)\n",
    "            modules.reverse()\n",
    "            \n",
    "            for idx in range(len(modules)):\n",
    "                modules[idx] = modules[idx].get_reverse()\n",
    "            self.sequential = nn.Sequential(*modules)\n",
    "        # Initialize by config (not implemented since we are using mirrored encoder/decoder)\n",
    "        else:\n",
    "            raise NotImplementedError('This decoder class is only implemented to be initialized by mirroring an encoder class')\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_config = config.encoder_config\n",
    "        encoder_config.in_channels = config.in_channels\n",
    "        self.encoder = Encoder(encoder_config)\n",
    "        \n",
    "        # Check for bottleneck input size by passing dummy input to encoder\n",
    "        dummy_input = torch.randn(1, config.in_channels, config.in_dimension[0], config.in_dimension[1])\n",
    "        out = self.encoder.forward(dummy_input)\n",
    "        out_dimension = list(out.size())\n",
    "        in_bottleneck = out_dimension[1] * out_dimension[2] * out_dimension[3]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Linear(in_bottleneck, config.bottleneck_width)\n",
    "        \n",
    "        # Bottleneck output reshaper\n",
    "        self.decoder_in_shape = out_dimension\n",
    "        self.bottleneck_reshape = nn.Conv2d(config.bottleneck_width, out_dimension[1], kernel_size=1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(self.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        out = self.encoder(x)\n",
    "        \n",
    "        # Reshape to fit bottleneck\n",
    "        encoder_out_shape = out.size() \n",
    "        flatten = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(flatten)\n",
    "        \n",
    "        # Reshape to fit decoder\n",
    "        out_reshaped = out.view(out.size()[0], out.size()[1], 1, 1)\n",
    "        out = self.bottleneck_reshape(out_reshaped)\n",
    "        out = nn.AdaptiveAvgPool2d((self.decoder_in_shape[2], self.decoder_in_shape[3]))(out)\n",
    "        \n",
    "        # Decoder\n",
    "        out = self.decoder(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a9890",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2da68",
   "metadata": {},
   "source": [
    "### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd0338d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict = {\n",
    "    'in_dimension': (dimension, dimension),\n",
    "    'in_channels': 3,\n",
    "    'encoder_config': {\n",
    "        'type': 'residual_blocks',\n",
    "        'compression_ratio': 2,\n",
    "        'features': [64, 128, 256, 512, 512, 512],\n",
    "        'main_layer_num': 3,\n",
    "        'shortcut_layer_num': 1,\n",
    "        'use_batchnorm': True,\n",
    "        'activation_func': {\n",
    "            'type': 'LeakyReLU',\n",
    "            'negative_slope': 0.1\n",
    "        },\n",
    "    },\n",
    "    'decoder_config': {\n",
    "        'mirror_encoder': True\n",
    "    },\n",
    "    'bottleneck_width': 256\n",
    "}\n",
    "\n",
    "\n",
    "test_config = ml_collections.ConfigDict(config_dict)\n",
    "\n",
    "autoencoder = AutoEncoder(test_config).cuda()\n",
    "autoencoder.forward(torch.randn(1, 3, 128, 128).type(dtype)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f2f61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset Flowers102\n",
       "    Number of datapoints: 1020\n",
       "    Root location: ./flower-102/train\n",
       "    split=train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc269a0e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3505c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# All hyperparameters to be tuned for the autoencoder network\n",
    "configs_dict = {\n",
    "    'in_dimension': [(dimension, dimension)],\n",
    "    'in_channels': [3],\n",
    "    'encoder_config': {\n",
    "        'type': ['residual_blocks'],\n",
    "        'compression_ratio': [2],\n",
    "        'features': [\n",
    "            [64, 128, 256, 512, 512, 512],\n",
    "            [64, 128, 256, 512, 512],\n",
    "            [64, 128, 256],\n",
    "            [32, 64, 128, 256, 512, 512, 512],\n",
    "            [32, 64, 128, 256, 512, 512],\n",
    "            [32, 64, 128, 256, 512],\n",
    "            [16, 32, 64, 128, 256, 512, 512],\n",
    "            [16, 32, 64, 128, 256, 512],\n",
    "            [16, 32, 64, 128, 256],\n",
    "        ],\n",
    "        'main_layer_num': [3, 2],\n",
    "        'shortcut_layer_num': [1],\n",
    "        'use_batchnorm': [True, False],\n",
    "        'activation_func':[\n",
    "            {\n",
    "                'type': 'LeakyReLU',\n",
    "                'negative_slope': 0.1\n",
    "            },\n",
    "            {\n",
    "                'type': 'LeakyReLU',\n",
    "                'negative_slope': 0.2\n",
    "            },\n",
    "            {\n",
    "                'type': 'ReLU',\n",
    "            }\n",
    "        ] \n",
    "            \n",
    "    },\n",
    "    'decoder_config': {\n",
    "        'mirror_encoder': [True]\n",
    "    },\n",
    "    'bottleneck_width': [512, 1024, 2048]\n",
    "}\n",
    "configs = ml_collections.ConfigDict(configs_dict)\n",
    "\n",
    "# Parse ConfigDict with hyperparameters to be tuned and output list of all ConfigDicts to be tested\n",
    "def generate_configs(configs):\n",
    "    config_list = [ml_collections.ConfigDict()]\n",
    "    for key in configs:\n",
    "        new_config_list = []\n",
    "        \n",
    "        current_key_configs = []\n",
    "        if isinstance(configs[key], list):\n",
    "            current_key_configs = configs[key]\n",
    "        elif isinstance(configs[key], ml_collections.config_dict.config_dict.ConfigDict):\n",
    "            current_key_configs = generate_configs(configs[key])\n",
    "        else:\n",
    "            raise TypeError(configs[key] + ' is neither a list nor an ml_collections.ConfigDict object')\n",
    "#         print(key, current_key_configs)\n",
    "        for key_config in current_key_configs:\n",
    "            for prev_config in config_list:\n",
    "                prev_config_copy = copy.deepcopy(prev_config)\n",
    "                prev_config_copy[key] = key_config\n",
    "                new_config_list.append(prev_config_copy)\n",
    "        config_list = new_config_list\n",
    "    return config_list\n",
    "\n",
    "config_list = generate_configs(configs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f0c5b1",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "164548ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configurations:   0%|                                                                          | 0/324 [00:00<?, ?it/s]\n",
      "Fold 1/5:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Fold 1/5:   0%|                                                               | 0/10 [00:07<?, ?it/s, Epoch Loss=0.363]\u001b[A\n",
      "Fold 1/5:  10%|█████▌                                                 | 1/10 [00:07<01:06,  7.35s/it, Epoch Loss=0.363]\u001b[A\n",
      "Fold 1/5:  10%|█████▌                                                 | 1/10 [00:13<01:06,  7.35s/it, Epoch Loss=0.206]\u001b[A\n",
      "Fold 1/5:  20%|███████████                                            | 2/10 [00:13<00:54,  6.86s/it, Epoch Loss=0.206]\u001b[A\n",
      "Fold 1/5:  20%|███████████                                            | 2/10 [00:20<00:54,  6.86s/it, Epoch Loss=0.168]\u001b[A\n",
      "Fold 1/5:  30%|████████████████▌                                      | 3/10 [00:20<00:47,  6.72s/it, Epoch Loss=0.168]\u001b[A\n",
      "Fold 1/5:  30%|████████████████▌                                      | 3/10 [00:27<00:47,  6.72s/it, Epoch Loss=0.134]\u001b[A\n",
      "Fold 1/5:  40%|██████████████████████                                 | 4/10 [00:27<00:40,  6.67s/it, Epoch Loss=0.134]\u001b[A\n",
      "Fold 1/5:  40%|██████████████████████▍                                 | 4/10 [00:33<00:40,  6.67s/it, Epoch Loss=0.11]\u001b[A\n",
      "Fold 1/5:  50%|████████████████████████████                            | 5/10 [00:33<00:33,  6.63s/it, Epoch Loss=0.11]\u001b[A\n",
      "Fold 1/5:  50%|███████████████████████████                           | 5/10 [00:40<00:33,  6.63s/it, Epoch Loss=0.0904]\u001b[A\n",
      "Fold 1/5:  60%|████████████████████████████████▍                     | 6/10 [00:40<00:26,  6.64s/it, Epoch Loss=0.0904]\u001b[A\n",
      "Fold 1/5:  60%|████████████████████████████████▍                     | 6/10 [00:46<00:26,  6.64s/it, Epoch Loss=0.0776]\u001b[A\n",
      "Fold 1/5:  70%|█████████████████████████████████████▊                | 7/10 [00:46<00:19,  6.64s/it, Epoch Loss=0.0776]\u001b[A\n",
      "Fold 1/5:  70%|█████████████████████████████████████▊                | 7/10 [00:53<00:19,  6.64s/it, Epoch Loss=0.0682]\u001b[A\n",
      "Fold 1/5:  80%|███████████████████████████████████████████▏          | 8/10 [00:53<00:13,  6.67s/it, Epoch Loss=0.0682]\u001b[A\n",
      "Fold 1/5:  80%|███████████████████████████████████████████▏          | 8/10 [01:00<00:13,  6.67s/it, Epoch Loss=0.0613]\u001b[A\n",
      "Fold 1/5:  90%|████████████████████████████████████████████████▌     | 9/10 [01:00<00:06,  6.74s/it, Epoch Loss=0.0613]\u001b[A\n",
      "Fold 1/5:  90%|█████████████████████████████████████████████████▌     | 9/10 [01:07<00:06,  6.74s/it, Epoch Loss=0.056]\u001b[A\n",
      "Fold 1/5: 100%|██████████████████████████████████████████████████████| 10/10 [01:07<00:00,  6.79s/it, Epoch Loss=0.056]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Configurations:   0%|                                                                          | 0/324 [01:10<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    103\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 104\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m fold_pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainloader)})\n\u001b[0;32m    107\u001b[0m fold_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "class AdaptiveAvgPool2dTransform:\n",
    "    def __init__(self, output_size):\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        tensor = self.to_tensor(img)\n",
    "        return self.pool(tensor)\n",
    "\n",
    "\n",
    "def load_images(root_dir, transform=None, num_images_per_category=1, selection_mode='first'):\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            AdaptiveAvgPool2dTransform((dimension, dimension))\n",
    "        ])\n",
    "    \n",
    "    # Load full dataset\n",
    "    full_dataset = ImageFolder(root=root_dir, transform=transform)\n",
    "    \n",
    "    # Select first or random 20 images per category\n",
    "    indices = []\n",
    "    targets = [item[1] for item in full_dataset.imgs]\n",
    "    unique_targets = set(targets)\n",
    "    \n",
    "    for target in unique_targets:\n",
    "        target_indices = [i for i, t in enumerate(targets) if t == target]\n",
    "        \n",
    "        if selection_mode == 'first':\n",
    "            selected_indices = target_indices[:num_images_per_category]\n",
    "        elif selection_mode == 'random':\n",
    "            selected_indices = random.sample(target_indices, min(num_images_per_category, len(target_indices)))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid selection_mode: {selection_mode}. Choose 'first' or 'random'.\")\n",
    "        \n",
    "        indices.extend(selected_indices)\n",
    "    \n",
    "    # Create a subset of the dataset based on the selected indices\n",
    "    subset_dataset = Subset(full_dataset, indices)\n",
    "    \n",
    "    return subset_dataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "learning_rate = 0.001\n",
    "num_folds = 5\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Define loss criterion\n",
    "criterion = torch.nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define KFold\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Load data\n",
    "root_dir = './Flowers299'\n",
    "selected_dataset = load_images(root_dir, num_images_per_category=1)\n",
    "\n",
    "# Store epoch losses for each config and fold\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "\n",
    "for config in tqdm(config_list, desc=\"Configurations\", position=0):\n",
    "    # Initialize a dict for current config\n",
    "    train_losses[str(config)] = {}\n",
    "    val_losses[str(config)] = {}\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(selected_dataset)):\n",
    "        # Fold progress bar\n",
    "        fold_pbar = tqdm(total=epochs, desc=f\"Fold {fold+1}/{num_folds}\", position=1, leave=False)\n",
    "        \n",
    "        # Sample elements randomly from a given list of ids, no replacement\n",
    "        train_subsampler = Subset(selected_dataset, train_ids)\n",
    "        val_subsampler = Subset(selected_dataset, val_ids)\n",
    "        \n",
    "        # Define data loaders for training and testing data for current fold\n",
    "        trainloader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "        valloader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Initialize the AutoEncoder with current config\n",
    "        model = AutoEncoder(config).cuda()\n",
    "        \n",
    "        # Define optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Store epoch losses for current fold\n",
    "        train_losses_for_fold = []\n",
    "        val_losses_for_fold = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train model\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for data in trainloader:\n",
    "                inputs, _ = data  # Ignoring category labels\n",
    "                inputs = inputs.type(dtype)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validate model\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in valloader:\n",
    "                    inputs, _ = data  # Ignoring category labels\n",
    "                    inputs = inputs.type(dtype)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            # Append epoch loss\n",
    "            train_losses_for_fold.append(train_loss / len(trainloader))\n",
    "            val_losses_for_fold.append(val_loss / len(valloader))\n",
    "            \n",
    "            # Update fold progress bar\n",
    "            fold_pbar.set_postfix({\"Epoch Val Loss\": val_loss / len(valloader)})\n",
    "            fold_pbar.update(1)\n",
    "        \n",
    "        fold_pbar.close()\n",
    "\n",
    "        # Update epoch losses for current fold\n",
    "        train_losses[str(config)][fold] = train_losses_for_fold\n",
    "        val_losses[str(config)][fold] = val_losses_for_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63c4af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "num_batch = 128\n",
    "batch_sizes = [4, 8, 16, 32, 64]\n",
    "time_taken = []\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    torch.cuda.empty_cache()\n",
    "    autoencoder = AutoEncoder(test_config).cuda()\n",
    "    start_time = time.time()\n",
    "    for batch in range(int(num_batch/batch_size)):\n",
    "        autoencoder.forward(torch.randn(batch_size, 3, 128, 128).type(dtype))\n",
    "    time_taken.append((batch_size, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca10289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 4: 0.6248235702514648\n",
      "Batch Size 8: 0.47405552864074707\n",
      "Batch Size 16: 0.43006157875061035\n",
      "Batch Size 32: 1.019606113433838\n",
      "Batch Size 64: 4.961598634719849\n"
     ]
    }
   ],
   "source": [
    "for data_point in time_taken:\n",
    "    print(f'Batch Size {data_point[0]}: {data_point[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73aeffde",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ac959",
   "metadata": {},
   "source": [
    "### WIP Stuffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
