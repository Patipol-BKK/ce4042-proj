{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6d16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download dataset\n",
    "dataset = datasets.Flowers102(root='./', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddccb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2118089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 468/468 [00:00<00:00, 9832.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder_decoder:\n",
       "  activation: ReLU\n",
       "  bottleneck: 16\n",
       "  depth: 4\n",
       "  max_epoch: 100\n",
       "  output_size: &id001 !!python/tuple\n",
       "  - 224\n",
       "  - 224\n",
       "  width: 64\n",
       "ffn:\n",
       "  activation: null\n",
       "  depth: null\n",
       "  dropout: null\n",
       "  width: null\n",
       "input_size: *id001\n",
       "max_epoch: 100\n",
       "num_classes: null\n",
       "pretrain: resnet18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.target_input_size = config.target_input_size\n",
    "        \n",
    "        self.layer = nn.ModuleList()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        x = nn.functional.adaptive_avg_pool2d(x, self.target_input_size)\n",
    "        \n",
    "# List of available pretrained resnets from pytorch\n",
    "class Pretrains():\n",
    "    resnet_versions = [\n",
    "        'resnet18',\n",
    "        'resnet34',\n",
    "        'resnet50',\n",
    "        'resnet101',\n",
    "        'resnet152'\n",
    "    ]\n",
    "    vgg_versions = [\n",
    "        'vgg11',\n",
    "        'vgg11_bn',\n",
    "        'vgg13',\n",
    "        'vgg13_bn',\n",
    "        'vgg16',\n",
    "        'vgg16_bn',\n",
    "        'vgg19',\n",
    "        'vgg19_bn'\n",
    "    ]\n",
    "            \n",
    "class PretrainBackbone(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet/VGG backbone\n",
    "        if config.pretrain in Pretrains.resnet_versions or config.resnet_version in Pretrains.vgg_versions:\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', config.pretrain, pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid ResNet/VGG Version. Please select from: ' \n",
    "                             + ', '.join(Pretrains.resnet_versions + Pretrains.vgg_versions))\n",
    "        \n",
    "        # Segments out only the backbone layers as list, unpacks, and load into nn.Sequential\n",
    "        backbone_layers = list(model.children())[:-1]\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "    \n",
    "def get_all_configs(training_config):\n",
    "    config_list = []\n",
    "    configs_num = len(training_config.pretrains) \\\n",
    "        * len(training_config.input_sizes) \\\n",
    "        * len(training_config.encoder_decoder.depths) \\\n",
    "        * len(training_config.encoder_decoder.widths) \\\n",
    "        * len(training_config.encoder_decoder.activations) \\\n",
    "        * len(training_config.encoder_decoder.bottlenecks) \\\n",
    "        * len(training_config.ffn.depths) \\\n",
    "        * len(training_config.ffn.widths) \\\n",
    "        * len(training_config.ffn.activations) \\\n",
    "        * len(training_config.ffn.dropouts)\n",
    "    pbar = tqdm('Generating Configs', total = configs_num, position = 0, leave = True)\n",
    "    # Pretrain hyperparameters\n",
    "    for pretrain in training_config.pretrains:\n",
    "        \n",
    "        for input_size in training_config.input_sizes:\n",
    "            # Encoder hyperparameters\n",
    "            for encoder_decoder_depth in training_config.encoder_decoder.depths:\n",
    "                for encoder_decoder_width in training_config.encoder_decoder.widths:\n",
    "                    for encoder_decoder_activation in training_config.encoder_decoder.activations:\n",
    "                        for encoder_decoder_bottleneck in training_config.encoder_decoder.bottlenecks:\n",
    "\n",
    "                            # FFN hyperparameters\n",
    "                            for ffn_depth in training_config.ffn.depths:\n",
    "                                for ffn_width in training_config.ffn.widths:\n",
    "                                    for ffn_activation in training_config.ffn.activations:\n",
    "                                        for ffn_dropout in training_config.ffn.dropouts:\n",
    "                                            # Initialize config dict\n",
    "                                            config = ml_collections.ConfigDict()\n",
    "\n",
    "                                            # Input\n",
    "                                            config.input_size = input_size\n",
    "\n",
    "                                            # Pretrain Network\n",
    "                                            config.pretrain = pretrain\n",
    "\n",
    "                                            # Encoder/Decoder Network (both use the same structure)\n",
    "                                            config.encoder_decoder = ml_collections.ConfigDict()\n",
    "                                            config.encoder_decoder.depth = encoder_decoder_depth\n",
    "                                            config.encoder_decoder.width = encoder_decoder_width\n",
    "                                            config.encoder_decoder.activation = encoder_decoder_activation\n",
    "                                            config.encoder_decoder.bottleneck = encoder_decoder_bottleneck\n",
    "                                            config.encoder_decoder.output_size =  input_size\n",
    "\n",
    "                                            config.encoder_decoder.max_epoch = training_config.encoder_decoder.max_epoch\n",
    "\n",
    "                                            # FFN\n",
    "                                            config.ffn = ml_collections.ConfigDict()\n",
    "                                            config.ffn.depth = ffn_depth\n",
    "                                            config.ffn.width = ffn_width\n",
    "                                            config.ffn.activation = ffn_activation\n",
    "                                            config.ffn.dropout = ffn_dropout\n",
    "\n",
    "                                            # Output\n",
    "                                            config.num_classes = training_config.num_classes\n",
    "\n",
    "                                            config.max_epoch = training_config.max_epoch\n",
    "\n",
    "                                            config_list.append(config)\n",
    "                                            pbar.update(1)\n",
    "    return config_list\n",
    "\n",
    "training_config = ml_collections.ConfigDict()\n",
    "training_config.input_sizes = [(224, 224)]\n",
    "training_config.pretrains = Pretrains.resnet_versions + Pretrains.vgg_versions\n",
    "\n",
    "training_config.encoder_decoder = ml_collections.ConfigDict()\n",
    "training_config.encoder_decoder.depths = [4, 8, 16]\n",
    "training_config.encoder_decoder.widths = [64, 256, 512]\n",
    "training_config.encoder_decoder.activations = ['ReLU']\n",
    "training_config.encoder_decoder.bottlenecks = [16, 32, 64, 128]\n",
    "training_config.encoder_decoder.max_epoch = 100\n",
    "\n",
    "training_config.ffn = ml_collections.ConfigDict()\n",
    "training_config.ffn.depths = [None]\n",
    "training_config.ffn.widths = [None]\n",
    "training_config.ffn.activations = [None]\n",
    "training_config.ffn.dropouts = [None]\n",
    "\n",
    "training_config.num_classes = None\n",
    "training_config.max_epoch = 100\n",
    "\n",
    "training_configs = get_all_configs(training_config)\n",
    "training_configs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cc9593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iraha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\iraha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\iraha/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 69.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "training_config = ml_collections.ConfigDict()\n",
    "training_config.input_sizes = [(224, 224)]\n",
    "training_config.pretrains = Pretrains.resnet_versions + Pretrains.vgg_versions\n",
    "\n",
    "training_config.encoder_decoder = ml_collections.ConfigDict()\n",
    "training_config.encoder_decoder.depths = [4, 8, 16]\n",
    "training_config.encoder_decoder.widths = [64, 256, 512]\n",
    "training_config.encoder_decoder.activations = ['ReLU']\n",
    "training_config.encoder_decoder.bottlenecks = [16, 32, 64, 128]\n",
    "training_config.encoder_decoder.max_epoch = 100\n",
    "\n",
    "training_config.ffn = ml_collections.ConfigDict()\n",
    "training_config.ffn.depths = [None]\n",
    "training_config.ffn.widths = [None]\n",
    "training_config.ffn.activations = [None]\n",
    "training_config.ffn.dropouts = [None]\n",
    "\n",
    "training_config.num_classes = None\n",
    "training_config.max_epoch = 100\n",
    "\n",
    "training_configs = get_all_configs(training_config)\n",
    "training_configs[0]\n",
    "import torchvision.models as models\n",
    "resnet = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78152d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\iraha/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b08bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\iraha/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\iraha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to C:\\Users\\iraha/.cache\\torch\\hub\\checkpoints\\vgg11_bn-6002323d.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 507M/507M [00:07<00:00, 66.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg11_bn', pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f0dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
