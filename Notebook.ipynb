{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6d16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "import ml_collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download dataset\n",
    "dataset = datasets.Flowers102(root='./', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b905f7",
   "metadata": {},
   "source": [
    "### Model Implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2118089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available pretrained resnets from pytorch\n",
    "class Pretrains():\n",
    "    resnet_versions = [\n",
    "        'resnet18',\n",
    "        'resnet34',\n",
    "        'resnet50',\n",
    "        'resnet101',\n",
    "        'resnet152'\n",
    "    ]\n",
    "    vgg_versions = [\n",
    "        'vgg11',\n",
    "        'vgg11_bn',\n",
    "        'vgg13',\n",
    "        'vgg13_bn',\n",
    "        'vgg16',\n",
    "        'vgg16_bn',\n",
    "        'vgg19',\n",
    "        'vgg19_bn'\n",
    "    ]\n",
    "\n",
    "class PretrainBackbone(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet/VGG backbone\n",
    "        if config.pretrain in Pretrains.resnet_versions or config.resnet_version in Pretrains.vgg_versions:\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', config.pretrain, pretrained=True)\n",
    "        else:\n",
    "            raise ValueError('Invalid ResNet/VGG Version. Please select from: ' \n",
    "                             + ', '.join(Pretrains.resnet_versions + Pretrains.vgg_versions))\n",
    "        \n",
    "        # Segments out only the backbone layers as list, unpacks, and load into nn.Sequential\n",
    "        backbone_layers = list(model.children())[:-1]\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class ActivationFunction(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ActivationFunction, self).__init__()\n",
    "        match config.type:\n",
    "            case 'LeakyReLU':\n",
    "                self.activation_func = nn.LeakyReLU(\n",
    "                    config.negative_slope,\n",
    "                    inplace = True\n",
    "                )\n",
    "            case 'ReLU':\n",
    "                self.activation_func = nn.ReLU(inplace = True)\n",
    "            case 'Softmax':\n",
    "                self.activation_func = nn.Softmax(dim = self.dim)\n",
    "            case _:\n",
    "                raise ValueError('Invalid activation function or not implemented')\n",
    "    def forward(self, x):\n",
    "        return self.activation_func(x)\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        modules = []\n",
    "        if config.layer_num < 1:\n",
    "            raise ValueError('Number of layers cannot be less than 1')\n",
    "        for layer_idx in range(config.layer_num):\n",
    "            # Conv2d\n",
    "            modules.append(nn.Conv2d(\n",
    "                config.in_channels if not layer_idx else config.out_channels,\n",
    "                config.out_channels,\n",
    "                kernel_size = 3,\n",
    "                padding = 1\n",
    "            ))\n",
    "            \n",
    "            # Batch Normalization\n",
    "            if config.use_batchnorm:\n",
    "                modules.append(nn.BatchNorm2d(config.out_channels))\n",
    "                \n",
    "            # Activation function, skip this step if skip_last_activation is True\n",
    "            if config.skip_last_activation and layer_idx == config.layer_num - 1:\n",
    "                break   \n",
    "            modules.append(ActivationFunction(config.activation_func))\n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "# Creates a mirrored Conv2dBlock\n",
    "class RevConv2dBlock(nn.Module):\n",
    "    def __init__(self, conv2d_block):\n",
    "        super(RevConv2dBlock, self).__init__()\n",
    "        \n",
    "        # Reverses module from conv2d_block\n",
    "        modules = list(conv2d_block.sequential)\n",
    "        modules.reverse()\n",
    "        module_iterator = iter(range(len(modules)))\n",
    "        for idx in module_iterator:\n",
    "            if isinstance(modules[idx], torch.nn.modules.batchnorm.BatchNorm2d):\n",
    "                \n",
    "                # Switch order of batch and conv2d\n",
    "                modules[idx], modules[idx + 1] = modules[idx + 1], modules[idx]\n",
    "                \n",
    "                # Swap conv2d with convtranspose2d\n",
    "                modules[idx] = nn.ConvTranspose2d(\n",
    "                    modules[idx].out_channels,\n",
    "                    modules[idx].in_channels,\n",
    "                    kernel_size = modules[idx].kernel_size,\n",
    "                    stride = modules[idx].stride,\n",
    "                    padding = modules[idx].padding\n",
    "                )\n",
    "                \n",
    "                modules[idx + 1] = nn.BatchNorm2d(modules[idx].out_channels)\n",
    "                \n",
    "                # Skip next index\n",
    "                next(module_iterator)\n",
    "            \n",
    "        if isinstance(modules[0], ActivationFunction):\n",
    "            activation_func = modules.pop(0)\n",
    "            modules.append(activation_func)\n",
    "            \n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "    \n",
    "class VGGBackboneBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VGGBackboneBlock, self).__init__()\n",
    "        config.skip_last_activation = False\n",
    "        \n",
    "        # Conv2d\n",
    "        self.conv2d_block = Conv2dBlock(config)\n",
    "        \n",
    "        # Maxpool\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size=config.compression_ratio, \n",
    "            stride=config.compression_ratio\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv2d_block(x)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n",
    "    \n",
    "    def get_reverse(self):\n",
    "        return RevVGGBackconeBlock(self)\n",
    "    \n",
    "class RevVGGBackconeBlock(nn.Module):\n",
    "    def __init__(self, vgg_backbone_block):\n",
    "        super(VGGBackboneBlock).__init__()\n",
    "        # To be implemented\n",
    "    def forward(self, x):\n",
    "        # To be implemented\n",
    "        return x\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Main Conv2d block\n",
    "        main_block_config = config\n",
    "        main_block_config.layer_num = config.main_layer_num\n",
    "        main_block_config.skip_last_activation = True\n",
    "        self.main_block = Conv2dBlock(main_block_config)\n",
    "        \n",
    "        # Shortcut Conv2d block, we leave self.shortcut_block as undefined if shortcut layer depth = 0\n",
    "        if config.shortcut_layer_num:\n",
    "            shortcut_block_config = config\n",
    "            shortcut_block_config.layer_num = config.shortcut_layer_num\n",
    "            shortcut_block_config.skip_last_activation = True\n",
    "            self.shortcut_block = Conv2dBlock(shortcut_block_config)\n",
    "            \n",
    "        self.activation_func = ActivationFunction(config.activation_func)\n",
    "        \n",
    "        # Optional maxpooling layer if compression_ratio is set\n",
    "        if hasattr(config, 'compression_ratio'):\n",
    "            self.maxpool = nn.MaxPool2d(\n",
    "                kernel_size=config.compression_ratio, \n",
    "                stride=config.compression_ratio\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.main_block(x)\n",
    "        if hasattr(self, 'shortcut_block'):\n",
    "            out += self.shortcut_block(x)\n",
    "        else:\n",
    "            out += x\n",
    "            \n",
    "        out = self.activation_func(out)\n",
    "            \n",
    "        if hasattr(self, 'maxpool'):\n",
    "            out = self.maxpool(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def get_reverse(self):\n",
    "        # Get reversed version\n",
    "        return RevResidualBlock(self)\n",
    "\n",
    "class RevResidualBlock(nn.Module):\n",
    "    def __init__(self, residual_block):\n",
    "        super(RevResidualBlock, self).__init__()\n",
    "        self.main_block = RevConv2dBlock(residual_block.main_block)\n",
    "        \n",
    "        if hasattr(residual_block, 'shortcut_block'):\n",
    "            self.shortcut_block = RevConv2dBlock(residual_block.shortcut_block)\n",
    "            \n",
    "        if hasattr(residual_block, 'maxpool'):\n",
    "            self.upsample = nn.Upsample(scale_factor=residual_block.maxpool.stride)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'upsample'):\n",
    "            x = self.upsample(x)\n",
    "        else:\n",
    "            x = x\n",
    "            \n",
    "        out = self.main_block(x)\n",
    "        \n",
    "        if hasattr(self, 'shortcut_block'):\n",
    "            out += self.shortcut_block(x)\n",
    "        else:\n",
    "            out += x\n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        modules = []\n",
    "        match config.type:\n",
    "            case 'residual_blocks':\n",
    "                for idx, block_feature in enumerate(config.features):\n",
    "                    if not idx:\n",
    "                        in_channels = config.in_channels\n",
    "                        out_channels = block_feature\n",
    "                    else:\n",
    "                        in_channels = config.features[idx - 1]\n",
    "                        out_channels = block_feature\n",
    "\n",
    "                    block_config = ml_collections.ConfigDict({\n",
    "                        'main_layer_num': config.main_layer_num,\n",
    "                        'shortcut_layer_num': config.shortcut_layer_num,\n",
    "                        'in_channels': in_channels,\n",
    "                        'out_channels': out_channels,\n",
    "                        'use_batchnorm': config.use_batchnorm,\n",
    "                        'activation_func': config.activation_func,\n",
    "                    })\n",
    "                    if hasattr(config, 'compression_ratio'):\n",
    "                        block_config.compression_ratio = config.compression_ratio\n",
    "\n",
    "                    modules.append(ResidualBlock(block_config))\n",
    "            case 'vgg_backbone_blocks':\n",
    "                # To be implemented\n",
    "                raise NotImplementedError('To be implemented')\n",
    "                \n",
    "        self.sequential = nn.Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, arg):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Initialize by mirroring encoder\n",
    "        if isinstance(arg, Encoder):\n",
    "            encoder = arg\n",
    "            modules = list(encoder.sequential)\n",
    "            modules.reverse()\n",
    "            \n",
    "            for idx in range(len(modules)):\n",
    "                modules[idx] = modules[idx].get_reverse()\n",
    "            self.sequential = nn.Sequential(*modules)\n",
    "        # Initialize by config (not implemented since we are using mirrored encoder/decoder)\n",
    "        else:\n",
    "            raise NotImplementedError('This decoder class is only implemented to be initialized by mirroring an encoder class')\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_config = config.encoder_config\n",
    "        encoder_config.in_channels = config.in_channels\n",
    "        self.encoder = Encoder(encoder_config)\n",
    "        \n",
    "        # Check for bottleneck input size by passing dummy input to encoder\n",
    "        dummy_input = torch.randn(1, config.in_channels, config.in_dimension[0], config.in_dimension[1])\n",
    "        out = self.encoder.forward(dummy_input)\n",
    "        out_dimension = list(out.size())\n",
    "        in_bottleneck = out_dimension[1] * out_dimension[2] * out_dimension[3]\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Linear(in_bottleneck, config.bottleneck_width)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(self.encoder)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        encoder_out_shape = out.size() \n",
    "        flatten = out.view(out.size(0), -1)\n",
    "        out = self.bottleneck(flatten)\n",
    "        reshaped = out.view(out.size()[0], out.size()[1], 1, 1)\n",
    "        out = self.decoder(reshaped)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be49ce0e",
   "metadata": {},
   "source": [
    "### Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "bb980e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5516, -0.2068, -0.4680,  ..., -0.8531, -0.4450,  0.1232],\n",
       "          [ 0.8301,  0.7162,  0.5984,  ..., -1.5186, -0.2426, -0.2329],\n",
       "          [ 0.8712,  1.0302,  1.0262,  ..., -0.5684,  0.7226,  0.5514],\n",
       "          ...,\n",
       "          [ 1.0588,  0.4694,  0.0622,  ...,  1.3015,  1.8702,  1.1376],\n",
       "          [ 1.1240,  0.0624, -0.1162,  ...,  1.1003,  1.2729,  1.0246],\n",
       "          [ 1.2998,  0.0777,  0.1123,  ...,  0.6273,  0.9946,  0.2985]],\n",
       "\n",
       "         [[ 0.0466,  0.8038,  0.8825,  ...,  0.8605,  0.5045,  0.8763],\n",
       "          [ 0.4250, -0.4446, -0.6750,  ...,  0.5111, -0.1032,  0.2369],\n",
       "          [-0.1207, -0.9937, -0.6350,  ...,  0.9737, -0.2861,  0.0639],\n",
       "          ...,\n",
       "          [ 1.1805,  1.4365,  1.4348,  ..., -0.0631, -0.1165,  0.6880],\n",
       "          [ 0.2076,  0.7495,  0.7422,  ...,  0.1660, -0.2923,  0.7185],\n",
       "          [-0.1095,  0.4830,  0.3474,  ..., -0.5860, -0.7834,  0.4724]],\n",
       "\n",
       "         [[-0.7530, -0.1011, -0.5831,  ..., -1.1542, -1.1337, -0.6581],\n",
       "          [ 0.0807,  0.0728, -0.3395,  ..., -0.2891, -0.8982, -0.8888],\n",
       "          [ 0.3040,  0.6055,  0.6394,  ..., -0.4869, -0.3255, -0.8409],\n",
       "          ...,\n",
       "          [-0.7113, -0.4473, -0.5902,  ..., -2.5433, -2.4858, -2.0015],\n",
       "          [-0.6829, -0.3645, -0.8601,  ..., -2.1493, -1.9289, -1.0697],\n",
       "          [-0.7774, -0.6079,  0.1020,  ..., -1.1192, -1.0699, -0.7661]]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict = {\n",
    "    'in_dimension': (224, 224),\n",
    "    'in_channels': 3,\n",
    "    'encoder_config': {\n",
    "        'type': 'residual_blocks',\n",
    "        'compression_ratio': 2,\n",
    "        'features': [64, 128, 256, 512, 512, 512],\n",
    "        'main_layer_num': 3,\n",
    "        'shortcut_layer_num': 1,\n",
    "        'use_batchnorm': True,\n",
    "        'activation_func': {\n",
    "            'type': 'LeakyReLU',\n",
    "            'negative_slope': 0.1\n",
    "        },\n",
    "    },\n",
    "    'decoder_config': {\n",
    "        'mirror_encoder': True\n",
    "    },\n",
    "    'bottleneck_width': 512\n",
    "}\n",
    "test_config = ml_collections.ConfigDict(config_dict)\n",
    "\n",
    "autoencoder = AutoEncoder(test_config)\n",
    "autoencoder.forward(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bcc0d",
   "metadata": {},
   "source": [
    "### WIP Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_configs(training_config):\n",
    "    config_list = []\n",
    "    configs_num = len(training_config.pretrains) \\\n",
    "        * len(training_config.input_sizes) \\\n",
    "        * len(training_config.autoencoder.depths) \\\n",
    "        * len(training_config.autoencoder.widths) \\\n",
    "        * len(training_config.autoencoder.activations) \\\n",
    "        * len(training_config.autoencoder.bottlenecks) \\\n",
    "        * len(training_config.ffn.depths) \\\n",
    "        * len(training_config.ffn.widths) \\\n",
    "        * len(training_config.ffn.activations) \\\n",
    "        * len(training_config.ffn.dropouts)\n",
    "    pbar = tqdm('Generating Configs', total = configs_num, position = 0, leave = True)\n",
    "    # Pretrain hyperparameters\n",
    "    for pretrain in training_config.pretrains:\n",
    "        \n",
    "        for input_size in training_config.input_sizes:\n",
    "            # Encoder hyperparameters\n",
    "            for autoencoder_depth in training_config.autoencoder.depths:\n",
    "                for autoencoder_width in training_config.autoencoder.widths:\n",
    "                    for autoencoder_activation in training_config.autoencoder.activations:\n",
    "                        for autoencoder_bottleneck in training_config.autoencoder.bottlenecks:\n",
    "\n",
    "                            # FFN hyperparameters\n",
    "                            for ffn_depth in training_config.ffn.depths:\n",
    "                                for ffn_width in training_config.ffn.widths:\n",
    "                                    for ffn_activation in training_config.ffn.activations:\n",
    "                                        for ffn_dropout in training_config.ffn.dropouts:\n",
    "                                            # Initialize config dict\n",
    "                                            config = ml_collections.ConfigDict()\n",
    "\n",
    "                                            # Input\n",
    "                                            config.input_size = input_size\n",
    "\n",
    "                                            # Pretrain Network\n",
    "                                            config.pretrain = pretrain\n",
    "\n",
    "                                            # Encoder/Decoder Network (both use the same structure)\n",
    "                                            config.autoencoder = ml_collections.ConfigDict()\n",
    "                                            config.autoencoder.depth = autoencoder_depth\n",
    "                                            config.autoencoder.width = autoencoder_width\n",
    "                                            config.autoencoder.activation = autoencoder_activation\n",
    "                                            config.autoencoder.bottleneck = autoencoder_bottleneck\n",
    "                                            config.autoencoder.output_size =  input_size\n",
    "\n",
    "                                            config.autoencoder.max_epoch = training_config.autoencoder.max_epoch\n",
    "\n",
    "                                            # FFN\n",
    "                                            config.ffn = ml_collections.ConfigDict()\n",
    "                                            config.ffn.depth = ffn_depth\n",
    "                                            config.ffn.width = ffn_width\n",
    "                                            config.ffn.activation = ffn_activation\n",
    "                                            config.ffn.dropout = ffn_dropout\n",
    "\n",
    "                                            # Output\n",
    "                                            config.num_classes = training_config.num_classes\n",
    "\n",
    "                                            config.max_epoch = training_config.max_epoch\n",
    "\n",
    "                                            config_list.append(config)\n",
    "                                            pbar.update(1)\n",
    "    return config_list\n",
    "\n",
    "training_config = ml_collections.ConfigDict()\n",
    "training_config.input_sizes = [(128), (224, 224), (360, 360)]\n",
    "training_config.pretrains = [None]\n",
    "\n",
    "training_config.autoencoder = ml_collections.ConfigDict()\n",
    "training_config.autoencoder.depths = [4, 8, 16, 32]\n",
    "training_config.autoencoder.widths = [64, 128, 256, 512]\n",
    "training_config.autoencoder.activations = ['ReLU', 'LeakyReLU']\n",
    "training_config.autoencoder.bottlenecks = [8, 16, 32, 64, 128]\n",
    "training_config.autoencoder.max_epoch = 100\n",
    "\n",
    "training_config.ffn = ml_collections.ConfigDict()\n",
    "training_config.ffn.depths = [None]\n",
    "training_config.ffn.widths = [None]\n",
    "training_config.ffn.activations = [None]\n",
    "training_config.ffn.dropouts = [None]\n",
    "\n",
    "training_config.num_classes = None\n",
    "training_config.max_epoch = 100\n",
    "\n",
    "training_configs = get_all_configs(training_config)\n",
    "training_configs[0]\n",
    "\n",
    "autoencoder_configs = ml_collections.ConfigDict()\n",
    "autoencoder_configs.autoencoder = ml_collections.ConfigDict()\n",
    "autoencoder_configs.autoencoder.layer_sizes = [\n",
    "    [(224, 224, 64), (112, 112, 128), (56, 56, 256), (28, 28, 512), (14, 14, 512), (7, 7, 512)],\n",
    "    [(224, 224, 64), (112, 112, 128), (56, 56, 256), (28, 28, 512), (14, 14, 512)],\n",
    "    [(224, 224, 32), (112, 112, 64), (56, 56, 128), (28, 28, 256), (14, 14, 512), (7, 7, 512)],\n",
    "    [(224, 224, 32), (112, 112, 64), (56, 56, 128), (28, 28, 256), (14, 14, 512)],\n",
    "    [(224, 224, 16), (112, 112, 32), (56, 56, 64), (28, 28, 128), (14, 14, 256), (7, 7, 512), (3, 3, 512)],\n",
    "    [(224, 224, 16), (112, 112, 32), (56, 56, 64), (28, 28, 128), (14, 14, 256), (7, 7, 512)],\n",
    "    [(224, 224, 16), (112, 112, 32), (56, 56, 64), (28, 28, 128), (14, 14, 256)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cc9593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 468/468 [00:00<00:00, 7173.71it/s]\n",
      "C:\\Users\\iraha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\iraha\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "training_config = ml_collections.ConfigDict()\n",
    "training_config.input_sizes = [(224, 224)]\n",
    "training_config.pretrains = Pretrains.resnet_versions + Pretrains.vgg_versions\n",
    "\n",
    "training_config.encoder_decoder = ml_collections.ConfigDict()\n",
    "training_config.encoder_decoder.depths = [4, 8, 16]\n",
    "training_config.encoder_decoder.widths = [64, 256, 512]\n",
    "training_config.encoder_decoder.activations = ['ReLU']\n",
    "training_config.encoder_decoder.bottlenecks = [16, 32, 64, 128]\n",
    "training_config.encoder_decoder.max_epoch = 100\n",
    "\n",
    "training_config.ffn = ml_collections.ConfigDict()\n",
    "training_config.ffn.depths = [None]\n",
    "training_config.ffn.widths = [None]\n",
    "training_config.ffn.activations = [None]\n",
    "training_config.ffn.dropouts = [None]\n",
    "\n",
    "training_config.num_classes = None\n",
    "training_config.max_epoch = 100\n",
    "\n",
    "training_configs = get_all_configs(training_config)\n",
    "training_configs[0]\n",
    "import torchvision.models as models\n",
    "resnet = models.resnet50(pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
